{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qxbn2yPygfHf"
      },
      "source": [
        "The Bloom_Filter functions takes our required input file and provides our desired output.\n",
        "\n",
        "To use it, call the function with:\n",
        "`lookup_count,FP_count=Bloom_Filter(inputfile,outputfile)`\n",
        "\n",
        "\n",
        "\n",
        "Note: In place of **inputfile** you can place your input csv file.\n",
        "\n",
        "**outputfile** will contain all the unique words of inputfile, with the duplicates removed. So thats our desired output.\n",
        "\n",
        "We need to put an empty .csv file at the desired location beforehand for **outputfile**.\n",
        "\n",
        "\n",
        "\n",
        "Also the function returns two output- **lookup_count** -> the number of lookups needed.\n",
        "\n",
        "**FP_count** -> the number of false positive by our algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "btU2BX4u42z6"
      },
      "outputs": [],
      "source": [
        "# Import required Libraries\n",
        "\n",
        "import csv\n",
        "import numpy as np\n",
        "import xxhash\n",
        "import mmh3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Pm6ODuNm5KbZ"
      },
      "outputs": [],
      "source": [
        "def Bloom_Filter(inp_filename,op_filename, exp_elem, hash_func_num, prob_false_pos = 0.8):\n",
        "\n",
        "     \n",
        "     \n",
        "     \n",
        "     lookup_count=0  # will store count of number of lookups\n",
        "     FP_count=0 # will store count of number of False Positives\n",
        "\n",
        "\n",
        "     \n",
        "     # Take our desired expected no. of element and probability of false positives from the users\n",
        "    #  exp_elem=float(input('What is the expected no. of Elements in our sample: '))\n",
        "    #  prob_false_pos=float(input('What is the probability of false positive: '))\n",
        "\n",
        "\n",
        "\n",
        "     # Calculating bit array side according to formula\n",
        "     bit_arr_size=round(-1*((exp_elem*np.log(prob_false_pos))/(np.square(np.log(2)))))\n",
        "     # Calculating no. of hash functions according to formula\n",
        "    #  hash_func_num= round((bit_arr_size/exp_elem)*np.log(2))\n",
        "    \n",
        "     print(\"number of hash functions\", hash_func_num)\n",
        "     print(\"bit array size\", bit_arr_size)\n",
        "\n",
        "\n",
        "     # Generating the hash functions. Here we use a random function: (sum_of_ascii_values_of_word)%bit_arr_size+i , where i refers to the index from 0\n",
        "    # to no.of hash function  . Suppose a particular bloom filter is 4 bit-array, and 2 hash functions so\n",
        "    # Eg: H1-> ascii(\"Hello\")%4+0\n",
        "    #     H2 -> ascii(\"Hello\")%4+1\n",
        "     def filters_to_have():\n",
        "            lst_fltr=[]\n",
        "            for i in range(hash_func_num):\n",
        "                # lst_fltr.append(xxhash.xxh32(word, seed=42).intdigest()%bit_arr_size+1)\n",
        "                lst_fltr.append(mmh3.hash(word, seed=0)%bit_arr_size)\n",
        "        # Note this function creates the required no. of hash functions with the above formula. The frmula is random and there is scope of improvement\n",
        "        # If needed to add own formula, the user can delete one of the formulas from lst_fltr and add their own formula\n",
        "            return lst_fltr\n",
        "\n",
        "\n",
        "\n",
        "    # Create the bit array with the required size and fill them inititally with 0. We use Python list to create the bit array\n",
        "     bloom_fltr=[0]*bit_arr_size\n",
        "\n",
        "    \n",
        "    # we will read each and every word from .csv file one-by-one and apply the bloom filter\n",
        "     with open(inp_filename, 'r') as file:\n",
        "         my_reader = csv.reader(file, delimiter=',')\n",
        "         next(file)\n",
        "         for row in my_reader:\n",
        "              word=row[0]\n",
        "              char_val_sum=sum([ord(z) for z in list(word)]) # getting sum of ascii vals of characters\n",
        "              lst_fltr=filters_to_have() # generating the hash values for each word\n",
        "              lst_fltr=[len(bloom_fltr)-1  if i>len(bloom_fltr)-1 else i for i in lst_fltr ] # the index/hash value should not exceed max size of bit array so clipping the ones which are exceeding\n",
        "\n",
        "              # Bloom Filter below main logic to find out the words present\n",
        "\n",
        "              # checking how many 0's are obtained in bit array for each hash values\n",
        "              # suppose there are 4 hash values. If sum_all is 4 then all are 0's\n",
        "              sum_all=sum([bloom_fltr[i]==0 for i in lst_fltr])\n",
        "              # even a single 0 in bit array for any of the hash value will prove the word is not present/non-duplicate so we will add them in output file\n",
        "              if sum_all>0:\n",
        "                    with open(op_filename, 'a', newline='') as csvfile:\n",
        "                          spamwriter1 = csv.writer(csvfile, delimiter=' ',quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
        "                          spamwriter1.writerow([word])\n",
        "                    for ij in lst_fltr:\n",
        "                        bloom_fltr[ij]=1\n",
        "              # if sum_all=0 then we need to lookup our outputfile to confirm that word is really present or it's a false positive\n",
        "              else:\n",
        "                    # lookup count increases by 1 since we are perfoming lookup\n",
        "                    lookup_count+=1\n",
        "                    # logic for checking whether the word is indeed present in the outputfile\n",
        "                    cc=0\n",
        "                    with open(op_filename, 'r') as file:\n",
        "                            my_reader = csv.reader(file, delimiter=',')\n",
        "                            # next(file)\n",
        "                            for row1 in my_reader:\n",
        "                              word1=row1[0]\n",
        "\n",
        "                              if word==word1:\n",
        "                                  cc+=1\n",
        "\n",
        "                    if cc>0:\n",
        "                        pass\n",
        "                    else:\n",
        "                           # if the case in false positive(FP) increment the FP_count by 1\n",
        "                           FP_count+=1\n",
        "                           # since the case is FP so originally the word is not present in the outputfile, so add it there\n",
        "                           with open(op_filename, 'a', newline='') as csvfile:\n",
        "                                 spamwriter3 = csv.writer(csvfile, delimiter=' ',quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
        "                                 spamwriter3.writerow([word])\n",
        "\n",
        "                    \n",
        "    \n",
        "\n",
        "\n",
        "     \n",
        "     \n",
        "     # Printing the lookup counts and false positive values\n",
        "     print('Number of False Positives: ', FP_count)\n",
        "     print('Number of Lookup Count: ', lookup_count)\n",
        "\n",
        "     # Returning the values of lookup counts and false positive\n",
        "     return [lookup_count,FP_count]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('streaming_words.csv')\n",
        "df = df.sample(frac=0.4).reset_index(drop=True)\n",
        "df = df.loc[:50000]\n",
        "df.to_csv(\"sample.csv\", index = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of hash functions 1\n",
            "bit array size 6967\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/home/tejas/Desktop/Academics/Sem7/SDS/StreamingDataSystems/Assignment2/bloomFilter.ipynb Cell 5\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/tejas/Desktop/Academics/Sem7/SDS/StreamingDataSystems/Assignment2/bloomFilter.ipynb#W4sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mword_drop\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.csv\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(n,i), \u001b[39m\"\u001b[39m\u001b[39ma\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m file:\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/tejas/Desktop/Academics/Sem7/SDS/StreamingDataSystems/Assignment2/bloomFilter.ipynb#W4sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/tejas/Desktop/Academics/Sem7/SDS/StreamingDataSystems/Assignment2/bloomFilter.ipynb#W4sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m Bloom_Filter(\u001b[39m\"\u001b[39;49m\u001b[39msample.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mword_drop\u001b[39;49m\u001b[39m{}\u001b[39;49;00m\u001b[39m.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49mformat(i), n, i, \u001b[39m0.8\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/tejas/Desktop/Academics/Sem7/SDS/StreamingDataSystems/Assignment2/bloomFilter.ipynb#W4sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTime taken for n = \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m and k = \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m is \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(n, i, time\u001b[39m.\u001b[39mtime()\u001b[39m-\u001b[39mstart_time))\n",
            "\u001b[1;32m/home/tejas/Desktop/Academics/Sem7/SDS/StreamingDataSystems/Assignment2/bloomFilter.ipynb Cell 5\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tejas/Desktop/Academics/Sem7/SDS/StreamingDataSystems/Assignment2/bloomFilter.ipynb#W4sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m my_reader \u001b[39m=\u001b[39m csv\u001b[39m.\u001b[39mreader(file, delimiter\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tejas/Desktop/Academics/Sem7/SDS/StreamingDataSystems/Assignment2/bloomFilter.ipynb#W4sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m \u001b[39m# next(file)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/tejas/Desktop/Academics/Sem7/SDS/StreamingDataSystems/Assignment2/bloomFilter.ipynb#W4sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m \u001b[39mfor\u001b[39;00m row1 \u001b[39min\u001b[39;00m my_reader:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tejas/Desktop/Academics/Sem7/SDS/StreamingDataSystems/Assignment2/bloomFilter.ipynb#W4sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m   word1\u001b[39m=\u001b[39mrow1[\u001b[39m0\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tejas/Desktop/Academics/Sem7/SDS/StreamingDataSystems/Assignment2/bloomFilter.ipynb#W4sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m   \u001b[39mif\u001b[39;00m word\u001b[39m==\u001b[39mword1:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import time\n",
        "for n in range(15000, 50000, 5000):\n",
        "    for i in range(1, 5, 1):\n",
        "        start_time = time.time()\n",
        "        with open(\"word_drop{}_{}.csv\".format(n,i), \"a\") as file:\n",
        "            pass\n",
        "        Bloom_Filter(\"sample.csv\", \"word_drop{}_{}.csv\".format(n,i), n, i, 0.8)\n",
        "        print(\"Time taken for n = {} and k = {} is {}\".format(n, i, time.time()-start_time))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1342.1089985370636"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "1694742260.6094174 - 1694740918.500419"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "-1012.3908450603485"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "1694742260.6094515  - 1694743273.0002966"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "-944.9537448883057"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "1694743273.0003104 - 1694744217.9540553"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "-922.2540216445923"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "1694744217.9540713 - 1694745140.208093"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "-912.0735535621643"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "1694745140.2081053 -  1694746052.281659"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3ZhTmTRvWQx",
        "outputId": "bb80d247-3ebe-4c7e-df95-6455362cd9fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of hash functions 3\n",
            "bit array size 1597949\n",
            "Number of False Positives:  32725\n",
            "Number of Lookup Count:  32818\n",
            "------------\n",
            "LOOKUP COUNT  32818\n",
            "------------\n",
            "FP_COUNT  32725\n"
          ]
        }
      ],
      "source": [
        "#n = 333425, p = 0.1\n",
        "lookup_count,FP_count=Bloom_Filter('streaming_words.csv','word_drop.csv')\n",
        "\n",
        "print('------------')\n",
        "print('LOOKUP COUNT ',lookup_count)\n",
        "print('------------')\n",
        "print('FP_COUNT ',FP_count)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4ELr1VswPkD"
      },
      "source": [
        "Notice the difference in outputs in thw two above for different choice of expected no. of elements and FP Probability."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goycdCEtxQSJ"
      },
      "source": [
        "The unique words are saved in **word_drop.csv**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PqC3Q8di3LT3",
        "outputId": "94a9ad3c-75e5-4dd9-da55-9427d5d4f0ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of hash functions 4\n",
            "bit array size 2078980\n",
            "Number of False Positives:  25290\n",
            "Number of Lookup Count:  25383\n",
            "------------\n",
            "LOOKUP COUNT  25383\n",
            "------------\n",
            "FP_COUNT  25290\n"
          ]
        }
      ],
      "source": [
        "#n = 333425, p = 0.05\n",
        "lookup_count,FP_count=Bloom_Filter('streaming_words.csv','word_drop_1.csv')\n",
        "\n",
        "print('------------')\n",
        "print('LOOKUP COUNT ',lookup_count)\n",
        "print('------------')\n",
        "print('FP_COUNT ',FP_count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#n = 333425, p = 0.01\n",
        "lookup_count,FP_count=Bloom_Filter('streaming_words.csv','word_drop_2.csv')\n",
        "\n",
        "print('------------')\n",
        "print('LOOKUP COUNT ',lookup_count)\n",
        "print('------------')\n",
        "print('FP_COUNT ',FP_count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#n = 333425, p = 0.005\n",
        "lookup_count,FP_count=Bloom_Filter('streaming_words.csv','word_drop_2.csv')\n",
        "\n",
        "print('------------')\n",
        "print('LOOKUP COUNT ',lookup_count)\n",
        "print('------------')\n",
        "print('FP_COUNT ',FP_count)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
